{
 "cells": [
  {
   "source": [
    "<a href=\"https://colab.research.google.com/github/scaomath/wustl-math450/blob/main/Homework/chw-3_YOUR_NAME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Classification problems\n",
    "Name:\n",
    "\n",
    "Wustlkey:\n",
    "\n",
    "Partner Name (if applicable):\n",
    "\n",
    "Partner Wustlkey (if applicable):\n",
    "\n",
    "### Submission instructions\n",
    "\n",
    "- Submit the modified python notebook as homework submission.\n",
    "- Group submission is enabled, you can submit this coding assignment with up to 1 teammate in our class. For instruction of how to do a group submission. Please refer to Canvas useful links.\n",
    "- You can google answers on StackOverflow, please attach the corresponding StackOverflow answer as comments. However, if the answer is converted to `torch` format, no credit will be awarded.\n",
    "- Do not change the number of cells! Please work in the cell provided. If we need extra cells for debugging and testing purposes, we can work at the end of this notebook, save everything as a backup for review, and delete the extra cells in the submitted version.\n",
    "\n",
    " \n",
    "\n",
    "### Instructions\n",
    "Do **not** use `for` loops for computational purpose in any of our solutions! We are allowed to use `for` loops to display figures etc.\n",
    "Efficieny will be graded as well. For example if a problem asks us generate an array from 0 to 9: then\n",
    "```python\n",
    "x = []\n",
    "for i in range(10):\n",
    "    x.append(i)\n",
    "```\n",
    "this will only result a partial credit while\n",
    "```python\n",
    "x = np.arange(10)\n",
    "```\n",
    "or\n",
    "```python\n",
    "x = torch.arange(10)\n",
    "```\n",
    "will yield a full score.\n",
    "\n",
    "### Problems\n",
    "Below are 4 problems that explore the needed data structure and operations for classification problems. Complete the coding tasks for credit.\n",
    "\n",
    "### Grading\n",
    "This homework has 4 problems, 5 points for each problem. The homework will be graded and the grade counts towards your course grade. \n",
    "\n",
    "## Coding environments and submission\n",
    "If we do not have `torch` installed on your computer, we have three ways to upload this notebook to [Google colab](https://colab.research.google.com/)：\n",
    "\n",
    "1. Open up Google Colab, choose `Upload` to upload this template and work there. After we have done working we can select `File->Download .ipynb`.\n",
    "2. Open up Google Colab, choose either `GitHub` or `Google Drive` to select the uploaded notebook in the corresponding website. After done working, we can sync the file to the corresponding GitHub or Google Drive copy.\n",
    "3. Use the \"Open in Colab\" button at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Me First\n",
    "import torch\n",
    "\n",
    "# import torchvision \n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# import packages that help us plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")"
   ]
  },
  {
   "source": [
    "## Dataset\n",
    "\n",
    "\n",
    "\"MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\"\n",
    "\n",
    "In the following cells, we will learn how to load and view this dataset for our toy models. \n",
    "\n",
    "Read more:[https://www.kaggle.com/c/digit-recognizer](https://www.kaggle.com/c/digit-recognizer)\n",
    "\n",
    "\n",
    "<a title=\"By Josef Steppan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:MnistExamples.png\"><img width=\"512\" alt=\"MnistExamples\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"/></a>\n",
    "\n",
    "\n",
    "---- \n",
    "This code is adopted from the pytorch examples repository. \n",
    "It is licensed under BSD 3-Clause \"New\" or \"Revised\" License.\n",
    "Source: https://github.com/pytorch/examples/\n",
    "LICENSE: https://github.com/pytorch/examples/blob/master/LICENSE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)\n",
    "### somehow torch.dataset malfunctioned after an update in March 2021\n",
    "### Facebook team issued a hotfix but apparently not loaded in the docker image\n",
    "### of Colab yet as of Mar 5, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data\n",
    "train = datasets.MNIST('../data', train=True, download=True, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = train.data[:10000].clone() # first 10000 samples\n",
    "target_new = train.targets[:10000].clone()"
   ]
  },
  {
   "source": [
    "## One-hot encoding of labels\n",
    "\n",
    "Encode labels as a \"one-hot\" numeric array. In binary classification, essentially a vector representation of the following indicator function:\n",
    "$$\n",
    "1_{\\{y = k\\}} = \\delta_{yk} = \\begin{cases}\n",
    "1 & \\text{when } y = k,\n",
    "\\\\[5pt]\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For example, if we have 4 classes `0, 1, 2, 3`, class `0` will be one-hot encoded as `[1, 0, 0, 0]`, and class `2` as `[0, 0, 1, 0]`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "## Problem 1\n",
    "Write a function `one_hot_encode()` to convert the class labels of MNIST dataset to a matrix of one-hot encoded vectors,\n",
    "each row represents the label of a sample. You can use `for` loop, but `for` loop will not be rewarded with full credits.\n",
    "An example for `for` loop implementation is as follows\n",
    " \n",
    "```python\n",
    "n_sample = len(data_new)\n",
    "n_class = 10\n",
    "one_hot_encoded_labels = torch.zeros((n_sample, 10), dtype=torch.float)\n",
    "for i in range(n_sample):\n",
    "    k = target_new[i]\n",
    "    one_hot_encoded_labels[i, k] = 1\n",
    "```\n",
    "\n",
    "There are various way of implementation the one-hot encoding without `for` loop, for example, expanding dimensions, using an identity matrix, etc. You can google StackOverflow to get answers, please attach the corresponding StackOverflow answer as comments if you have done so. However, if the answer has not been converted to `torch` format, no credit will be awarded. Only the usage of a `torch` function is allowed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode():\n",
    "    '''\n",
    "    Replace this with your code\n",
    "    '''\n",
    "    return None\n",
    "\n",
    "\n",
    "print(one_hot_encode(target_new[:10]))\n",
    "# expected output\n",
    "# tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "#         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "#         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "#         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "#         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "#         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
   ]
  },
  {
   "source": [
    "## Softmax function\n",
    "\n",
    "Softmax function is a tool for multi-class classification problem. Suppose that we know for a certain sample, its label $y\\in \\{1,\\dots, K\\}$ where $K$ is the number of all possible classes in the samples. For a sample image flattened $\\mathbf{x}\\in \\mathbb{R}^n$ (in MNIST it is a 28x28 image flattened to a `(784,)` array), we can estimate the probability that $P(y=k|\\mathbf{x})$ using our neural network's output as follows:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "P(y = 1 | \\mathbf{x}; W) \\\\\n",
    "P(y = 2 | \\mathbf{x}; W) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = K | \\mathbf{x}; W)\n",
    "\\end{pmatrix}\n",
    "=\\text{Softmax} (\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_K)\n",
    ":= \\frac{1}{ \\sum_{j=1}^{K}{\\exp\\big(\\hat{y}_j\\big) }}\n",
    "\\begin{pmatrix}\n",
    "\\exp(\\hat{y}_1) \\\\\n",
    "\\exp(\\hat{y}_2) \\\\\n",
    "\\vdots \\\\\n",
    "\\exp(\\hat{y}_K) \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "where we have $K$ outputs from a neutral network (preferably after a ReLU activation in the last layer), $\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_K$ are outputs,  and the factor $\\sum_{j=1}^{K}{\\exp\\big(\\hat{y}_j\\big)}$ normalizes the results to be a probability (each entry is between 0 to 1, and summing up to 1)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Problem 2\n",
    "\n",
    "Implement a vectorized `softmax()` function by ourselves without referring to `torch.nn.functional.softmax()`, i.e., for a `(N, 10)` matrix which contains the outputs of neural network, the function should return the same matrix with the `proba` produced in the following `for` loop:\n",
    "\n",
    "```python\n",
    "N = len(data_new)\n",
    "y_hat = torch.randn((N, 10)) # generate a random guess\n",
    "proba = torch.zeros_like(y_hat)\n",
    "for i in range(N):\n",
    "    exp_y_hat = torch.exp(y_hat[i])\n",
    "    sum_exp = torch.sum(exp_y_hat)\n",
    "    proba[i] = exp_y_hat/sum_exp\n",
    "\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax():\n",
    "    '''\n",
    "    Replace this with your code\n",
    "    \n",
    "    Input:\n",
    "        - yhat: (N, K) matrix\n",
    "            N: number of samples\n",
    "            K: number of classes\n",
    "    Output:\n",
    "        - proba: (N, K) matrix\n",
    "    '''\n",
    "    return None\n",
    "\n",
    "\n",
    "N = len(data_new)\n",
    "y_hat = torch.randn((N, 10))\n",
    "proba = softmax(y_hat)\n",
    "print(torch.allclose(proba.sum(axis=1), torch.ones(N))) # expected output is True"
   ]
  },
  {
   "source": [
    "## Generating the prediction \n",
    "\n",
    "After we get the probability, to predict which class this sample belongs to, we simply choose the biggest entry as the class.\n",
    "For example, if we have a 4-class classification problem, the predicted probability for a sample can be \n",
    "- `[0.3, 0.1, 0.4, 0.2]`,  the `pred` for this sample is of class `2`.\n",
    "- `[0.05, 0.8, 0.05, 0.1]`,  the `pred` for this sample is of class `1`.\n",
    "- `[0.6, 0.1, 0.2, 0.1]`,  the `pred` for this sample is of class `0`.\n",
    "\n",
    "## Metrics for classification problem\n",
    "\n",
    "- Accuracy: the simplest metric in classification, if the label(s) predicted for a sample match the true label(s) associated with this sample, we say this is a correct prediction, otherwise incorrect. The accuracy metric is simply:\n",
    "$$\n",
    "\\text{Acc} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{|y^{(i)}\\cap \\hat{y}(x^{(i)}; W)|}{| \\hat{y}(x^{(i)}; W)|} = \\frac{\\# \\text{Correct}}{\\# \\text{Total}}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Problem 3\n",
    "\n",
    "Write a vectorized function `proba_to_pred()` that converts the predicted probability matrix in problem 2 to prediction vector, i.e., for each row of the probability matrix, find where its maximum entry is (the index of the maximum entry in that row).\n",
    "\n",
    "## Problem 4\n",
    "Write a vectorized function `accuracy_score()` function to compute the accuracy between predicted labels and the true labels. \n",
    "A `for` loop implementation can be follows: suppose `pred` is the predicted class vector that has same shape with `target_new` we defined earlier.\n",
    "\n",
    "```python\n",
    "N = len(target_new)\n",
    "counter = 0\n",
    "for i in range(N):\n",
    "    counter += (target_new[i] == pred[i])\n",
    "\n",
    "acc = counter/N\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_to_pred():\n",
    "    '''\n",
    "    Replace this with your code\n",
    "    \n",
    "    Input:\n",
    "        - predicted probability matrix: (N, K)\n",
    "            N: # samples\n",
    "            K: # of classes\n",
    "        \n",
    "    Output:\n",
    "        - pred: predicted classes (N, )\n",
    "    '''\n",
    "\n",
    "    return None\n",
    "\n",
    "def accuracy_score():\n",
    "    '''\n",
    "    Replace this with your code\n",
    "    \n",
    "    Input:\n",
    "        - predicted labels: one-hot encoded prediction\n",
    "        - true labels: one-hot encoded true labels\n",
    "\n",
    "    Output:\n",
    "        - accuracy score above\n",
    "    '''\n",
    "    return None\n",
    "\n",
    "\n",
    "y_hat = torch.randn((target_new.size(0), 10)).softmax(dim=1) # random guess then softmax\n",
    "pred = proba_to_pred(y_hat) # your implemented function in problem 3\n",
    "acc = accuracy_score(pred, target_new) \n",
    "\n",
    "print(pred.min(), pred.max()) # expected output: 0 9\n",
    "print(acc) # expected output approximately 0.1 (random guess's correct rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}