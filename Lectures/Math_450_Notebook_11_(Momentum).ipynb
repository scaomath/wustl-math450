{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Math 450 Notebook 11 (Momentum).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbt9sa/0lDIiSWXP08+90m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaomath/wustl-math450/blob/main/Lectures/Math_450_Notebook_11_(Momentum).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWgIuaGfIBj2"
      },
      "source": [
        "# Coding lecture 11 of Math 450\n",
        "\n",
        "## Last couple of weeks\n",
        "- A complete pipeline of training a machine learning model\n",
        "- Validation\n",
        "\n",
        "## Today\n",
        "- Add momentum to the formula."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9wNb-1mJ94C"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import Optimizer\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"dark\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPSmTtOBKGiu"
      },
      "source": [
        "train = datasets.MNIST(root='./', \n",
        "                       train=True, \n",
        "                       download=True, \n",
        "                       transform = transforms.ToTensor());\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=8) \n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_size: int = 28*28,\n",
        "                 output_size: int = 10):\n",
        "        super(MLP, self).__init__() \n",
        "        self.linear0 = nn.Linear(input_size, 256)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.linear1 = nn.Linear(256, output_size)\n",
        "        self.dropout = nn.Dropout(0.1) \n",
        "        # 10% of the weight does not get updated: dropout\n",
        "        \n",
        "    def forward(self, x): \n",
        "        x = x.view(x.size(0), -1) \n",
        "        x1 = self.linear0(x)\n",
        "        a1 = self.activation(x1)\n",
        "        output = self.linear1(a1)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikMGFZJgKYYP"
      },
      "source": [
        "class SGD(Optimizer):\n",
        "    \"\"\"\n",
        "      Implements the SGD with momentum simplified \n",
        "      from the torch official one for Math 450 WashU\n",
        "      \n",
        "      Args:\n",
        "          params (iterable): iterable of parameters to optimize or dicts defining\n",
        "              parameter groups\n",
        "          lr (float): learning rate\n",
        "          weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "          nesterov (bool, optional): whether to use Nesterov's momentum (default: False)\n",
        "      \n",
        "      For final project:\n",
        "          update a version with nesterov's momentum in it\n",
        "          \n",
        "      Example:\n",
        "          >>> optimizer = SGD(model.parameters(), lr=1e-2)\n",
        "          >>> optimizer.zero_grad()\n",
        "          >>> loss_fn(model(input), target).backward()\n",
        "          >>> optimizer.step()\n",
        "      \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, momentum=0, dampening=0,\n",
        "                 weight_decay=0, \n",
        "                 nesterov=False,\n",
        "                 ):\n",
        "      defaults = dict(lr=lr, \n",
        "                      momentum=momentum, \n",
        "                      dampening=dampening,\n",
        "                      weight_decay=weight_decay,\n",
        "                      nesterov=nesterov,\n",
        "                      )\n",
        "      super(SGD, self).__init__(params, defaults)\n",
        "      \n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buffer = param_state['momentum_buffer'] = torch.zeros_like(p.data)\n",
        "                        buffer.mul_(momentum).add_(d_p)\n",
        "                    else:\n",
        "                        buffer = param_state['momentum_buffer']\n",
        "                        buffer.mul_(momentum).add_(1 - dampening, d_p)\n",
        "                    d_p = buffer\n",
        "\n",
        "                p.data = p.data - group['lr']*d_p\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3j055o9KtP2"
      },
      "source": [
        "model = MLP() # initialize the model\n",
        "loss_func = nn.CrossEntropyLoss() # set up the loss\n",
        "# crossentropyloss is for the case of a balanced classification problem\n",
        "epochs = 2\n",
        "learning_rate = 1e-3\n",
        "optimizer = SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGFev_UNL84b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMVlM4y4M-x8"
      },
      "source": [
        "X = train.data.float()[:10000]\n",
        "y = train.targets[:10000]\n",
        "print(X.size(), y.size())\n",
        "X_tr, X_val, y_tr, y_val = \\\n",
        "train_test_split(X, y, random_state=0, train_size=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi7KfZ3-YHUm"
      },
      "source": [
        "print(X_tr.size(), X_val.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HxCd_QpNWj5"
      },
      "source": [
        "train_set = TensorDataset(X_tr, y_tr)\n",
        "train_loader = DataLoader(train_set, batch_size=32)\n",
        "\n",
        "valid_set = TensorDataset(X_val, y_val)\n",
        "val_loader = DataLoader(valid_set, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YojEUAzcYZEP"
      },
      "source": [
        "# pipeline\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    model.train() # formalism, useful when we have dropout\n",
        "    \n",
        "    loss_vals = []\n",
        "    acc_on_valid = []\n",
        "    \n",
        "    with tqdm(total=len(train_loader)) as pbar: # progress bar\n",
        "      for data, targets in train_loader:\n",
        "        \n",
        "        # forward pass\n",
        "        outputs = model(data)\n",
        "        \n",
        "        # loss function\n",
        "        loss = loss_func(outputs, targets)\n",
        "        \n",
        "        # record loss function values .item()\n",
        "        loss_vals.append(loss.item())\n",
        "        \n",
        "        # clean the gradient from last iteration\n",
        "        # param.grad is not zero in last iteration\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # backprop\n",
        "        # autograd\n",
        "        loss.backward()\n",
        "        \n",
        "        # stochastic gradient descent\n",
        "        # no with torch.no_grad(): block, param operation is using .data\n",
        "        optimizer.step()\n",
        "        \n",
        "        # check accuracy (add validation here)\n",
        "        with torch.no_grad():\n",
        "           for x, y in val_loader:\n",
        "             yhat = model(x)\n",
        "             yhat = yhat.argmax(dim=-1)\n",
        "             acc = (yhat == y).float().mean()\n",
        "             acc_on_valid.append(acc)\n",
        "\n",
        "        # tqdm template\n",
        "        desc = f\"epoch: [{epoch+1}/{epochs}] loss: {np.mean(loss_vals):.2f}\"\n",
        "        pbar.set_description(desc)\n",
        "        pbar.update()\n",
        "    print(f\"accuracy on validation: {np.mean(acc_on_valid):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG1MAtj2Q1JA"
      },
      "source": [
        "# Explicit gradient checking\n",
        "\n",
        "\n",
        "$$f = 3a^3 - b^2$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUjFHly0aN_3"
      },
      "source": [
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)\n",
        "\n",
        "f = (3*a**3 - b**2).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWehCH8xRloX"
      },
      "source": [
        "optimizer = SGD([a, b], lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT19xDQvRooW"
      },
      "source": [
        "f.backward()\n",
        "optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}